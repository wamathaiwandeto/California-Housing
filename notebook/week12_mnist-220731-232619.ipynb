{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX89fleeiG0j"
      },
      "source": [
        "# Monday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXljSsfriG0m"
      },
      "source": [
        "The most common supervised learning tasks are regression (predicting values) and classification (predicting classes). Last week we explored a regression task, predicting housing values, using various algorithms. Now we will turn our attention to classification\n",
        "systems.\n",
        "\n",
        "## 1. Business Understanding\n",
        "In this week, we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied\n",
        "so much that it is often called the “Hello World” of Machine Learning: whenever people come up with a new classification algorithm, they are curious to see how it will perform on MNIST. Whenever someone learns Machine Learning, sooner or later they tackle MNIST. \n",
        "\n",
        "\n",
        "The task you are asked to perform is to build a model that classifies handwritten digits, given the handwritten images.  \n",
        "\n",
        "For learning purposes, we will split the Modeling phase into two parts. In the first part, we will simplify the problem and train a model to predict one digit – for example the number 5. This “5-detector” is an example of a binary classifier, capable of distinguishing between two classes, 5 and not-5. In the second part, we will explore multiclass classifiers that predicts all classes.   \n",
        "\n",
        "### 1.1 Frame the problem\n",
        "Since each image is labeled with the digit it represents, the problem is a supervised learning problem. Furthermore, since you are asked to classify digits, the problem is a classification problem.  \n",
        "\n",
        "In case of predicting the classes “5” and “not 5”, the problem is binary classification problem. In case of predicting all classes, the problem is a multiclass classification problem.  \n",
        "\n",
        "Remark: In case of a binary classification problem, the main  goal is to train a model that identifies the digit 5. This class is referred to as the positive class, while the not-5 class is referred to as the negative class.  \n",
        "\n",
        "### 1.2 Select a Performance Measure \n",
        "\n",
        "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will take a good amount of time introducing you to the four fundamental performance measures for classifiers.  \n",
        "\n",
        "#### 1.2.1 Classification Accuracy \n",
        "\n",
        "Classification Accuracy, or accuracy for short, might be the most intuitive metric. It shows the ratio of correct predictions to all predictions: \n",
        "- Accuracy = number of correct predictions / number of all predictions\n",
        "\n",
        "It works well ONLY if there are equal number of samples belonging to each class! Why this is the case, is best explained with an example.  \n",
        "\n",
        "Let us consider the spam email example, that is, imagine you are designing a model to detect spam emails. An ordinary email address receives very few spam emails compared to the other emails. Thus, the dataset used for training is likely to be unbalanced. \n",
        "\n",
        "Let’s say the ratio of spam emails and regular ones in the dataset is 5 to 95. If a model predicts every email as not spam, it will have an accuracy of 95% which actually sounds good. However, it is a model that does nothing (always predicts not spam). \n",
        "\n",
        "Furthermore, mistakes on spam and other emails should be handled differently. It is not much of a problem to miss a spam email and let it go to the inbox. However, it could have severe consequences to mark an important email as spam. \n",
        "\n",
        "The classification accuracy does not provide us the flexibility that we need for differentiating mistakes on spam and other emails. Next we will introduce a more “flexible” metric to evaluate the performance of a classification model.  \n",
        "\n",
        "#### 1.2.2 Confusion Matrix\n",
        "\n",
        "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model. In particular, it provides an overview of the model performance on different classes separately. \n",
        "\n",
        "Let us assume we have a binary classification problem. We have some samples belonging to two classes : YES or NO. Also, we have our own classifier which predicts a class for a given input sample. On testing our model on 165 samples, we get the following result. \n",
        "\n",
        "|           |Prediction: NO | Prediction Yes |\n",
        "|:----------|:-------------:|---------------:|\n",
        "|**Acutal: NO**| 50            | 10             |\n",
        "|**Acutal: YES**| 5             | 100            |\n",
        "\n",
        "In this example, NO is the positive class (first row and first column) while YES is the negative class. There are 4 important terms : \n",
        "\n",
        "- True Positives (TN): The cases in which we predicted YES and the actual output was also YES. \n",
        "\n",
        "- True Negatives (TP): The cases in which we predicted NO and the actual output was NO. \n",
        "\n",
        "- False Positives (FP): The cases in which we predicted YES and the actual output was NO. \n",
        "\n",
        "- False Negatives (FN): The cases in which we predicted NO and the actual output was YES. \n",
        "\n",
        "Both TP and TN are correct predictions so we expect them to be high compared to the FP and FN. \n",
        "\n",
        "In general, a confusion matrix for a binary classification problem has the following form:  \n",
        "\n",
        "|            |Negative       | Positive |\n",
        "|:-----------|:-------------:|---------------:|\n",
        "|**Negative**| TN            | FP             |\n",
        "|**Positive**| FN            | TP             |\n",
        "\n",
        "Accuracy for the matrix can be calculated by taking average of the values lying across the “main diagonal” i.e., \n",
        "- Accuracy = (True Positive + True Negative) / total sample\n",
        "\n",
        "[Machine Learning Fundamentals: The Confusion Matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o)\n",
        "\n",
        "#### 1.2.3 Precision and Recall  \n",
        "The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric.  \n",
        "\n",
        "An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier. Precision corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. It is defined as: \n",
        "- Precision = TP / (TP + FP)\n",
        "\n",
        "A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or true positive rate (TPR). \n",
        "\n",
        "Recall is the ratio of positive instances that are correctly detected by the classifier and is defined as:\n",
        "- Recall = TP / (TP + FN)\n",
        "\n",
        "#### 1.2.4 F1 Score \n",
        "\n",
        "It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. \n",
        "\n",
        "F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances). \n",
        "\n",
        "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. \n",
        "\n",
        "The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.  \n",
        "\n",
        "For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught).  \n",
        "\n",
        "Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeoff. We will explore this tradeoff in more depth soon. \n",
        "\n",
        "[Precision, Recall, & F1 Score Intuitively Explained](https://www.youtube.com/watch?v=8d3JbbSj-I8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RmXqOMRiG0t"
      },
      "source": [
        "## 2. Data Understanding\n",
        "\n",
        "Data understanding typically involves the following steps: \n",
        "\n",
        "- Determine what data is needed and collect the data if not available \n",
        "- Explore data \n",
        "- Verify data quality \n",
        "\n",
        "Regarding the first step; Scikit-Learn provides many helper functions to download popular datasets. MNIST is one of them. The following code fetches the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQNUkWdZiG0u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRKojVbEiG0w"
      },
      "outputs": [],
      "source": [
        "mnist = mnist = fetch_openml('mnist_784', version=1)\n",
        "type(mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CRusjKhiG0x"
      },
      "outputs": [],
      "source": [
        "mnist.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNhb6KfriG0y"
      },
      "source": [
        "Datasets loaded by Scikit-Learn generally have a similar dictionary structure including:\n",
        "- A DESCR key describing the dataset\n",
        "- A data key containing an array with one row per instance and one column per feature\n",
        "- A target key containing an array with the labels\n",
        "\n",
        "Next we store the features as a pandas DataFrame and the target variable as a pandas Series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY1be3NRiG0z"
      },
      "outputs": [],
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "type(X), type(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWZ0WITjiG00"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myBkS48uiG02"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9FI_MALiG02"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNCIc3biG03"
      },
      "source": [
        "There are 70,000 images, and each image has 784 features. This is because each image is 28×28 = 784 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
        "\n",
        "Let’s take a peek at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s imshow() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p47bwU4GiG04"
      },
      "outputs": [],
      "source": [
        "some_digit = X.iloc[0,:].values\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "\n",
        "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlhiFmHuiG05"
      },
      "source": [
        "This looks like a 5, and indeed that’s what the label tells us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIL2JapMiG06"
      },
      "outputs": [],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmZoc2IFiG06"
      },
      "source": [
        "Note that the label is a string. We prefer numbers, so let’s cast y to integers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVM7a_FniG07"
      },
      "outputs": [],
      "source": [
        "y = y.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4wJTcUAiG08"
      },
      "source": [
        "Let us check for missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2-X7Kj8iG08"
      },
      "outputs": [],
      "source": [
        "X.isnull().sum().sum() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxGx4lMLiG09"
      },
      "outputs": [],
      "source": [
        "# or \n",
        "X.isnull().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XKmTdgNiG0-"
      },
      "source": [
        "There are no missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr9L2aQsiG0-"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "There are no categorical variables and no missing values. Since the range of each row, that is, of each image ranges between 0 and 255, there are also no outliers. Thus, the only thing left is to split the data into training and testing.\n",
        "\n",
        "The MNIST dataset is actually already split into a training set (the first 60.000 images) and a test set (the last 10,000 images):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZlYNxBWiG0-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7H1C-AJiG0_"
      },
      "source": [
        "The training set is already shuffled for us, which is good as this guarantees that all cross-validation folds will be similar (you don’t want one fold to be missing some digits). Moreover, some learning algorithms are sensitive to the order of the training\n",
        "instances, and they perform poorly if they get many similar instances in a row. Shuffling the dataset ensures that this won’t happen.\n",
        "\n",
        "Remark: Shuffling may be a bad idea in some contexts — for example, if you are working on time series data, that is, data is time dependent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IJD3CxHiG0_"
      },
      "source": [
        "# Tuesday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSeGxnOciG1A"
      },
      "source": [
        "## 4. Modeling\n",
        "## 4.1 Binary Classifier\n",
        "Let’s simplify the problem for now and only try to identify one digit — for example, the number 5. This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\n",
        "this classification task:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MJ_wJTniG1A"
      },
      "outputs": [],
      "source": [
        "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\n",
        "y_test_5 = (y_test == 5) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxLdVBCjiG1B"
      },
      "source": [
        "Okay, now let’s pick a classifier and train it. A good place to start is with a **Stochastic Gradient Descent (SGD)** classifier, using Scikit-Learn’s SGDClassifier class. This classifier has the advantage of being capable of handling very large datasets efficiently.\n",
        "For further information regarding the SGD, please watch:\n",
        "\n",
        "[Stochastic Gradient Descent, Clearly Explained!!!](https://www.youtube.com/watch?v=vMh0zPT0tLI&t=4s)\n",
        "\n",
        "Let’s create an SGDClassifier and train it on the whole training set: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o57ElTImiG1C"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Z7gb6MiG1C"
      },
      "source": [
        "Remark: The SGDClassifier relies on randomness during training (hence the name “stochastic”). If you want reproducible results, you should set the random_state parameter.\n",
        "\n",
        "Now you can use it to detect images of the number 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOhYP40OiG1D"
      },
      "outputs": [],
      "source": [
        "sgd_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxyJZt56iG1E"
      },
      "source": [
        "The classifier guesses that this image represents a 5 (True). Looks like it guessed right in this particular case! Now, let’s evaluate this model’s performance.\n",
        "\n",
        "As we learned yesterday, evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this week on this topic.\n",
        "\n",
        "### 4.1.1 Measuring Accuracy Using Cross-Validation\n",
        "A good way to evaluate a model is to use cross-validation, just as you did last week. Let’s use the cross_val_score() function to evaluate your SGDClassifier model using K-fold cross-validation, with three folds. Remember that K-fold crossvalidation means splitting the training set into K-folds (in this case, three), then making predictions and evaluating them on each fold using a model trained on the remaining folds:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkJhV3hdiG1E"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wTAzMWPiG1F"
      },
      "source": [
        "Wow! Above 94% accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn’t it? Well, before you get too excited, let’s look at how many digits from the training set correspond to 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ-WXOqNiG1L"
      },
      "outputs": [],
      "source": [
        "y_train_5. / y_train.shape[0] * 100 # 9% of the digits are 5, that is, 91% of them are not 5!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuNaiai_iG1L"
      },
      "source": [
        "9% of the digits are 5, that is, 91% of them are not 5! This means, that if a model always classfies a digit as non-5, it will be right 91% of time! \n",
        "\n",
        "This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).\n",
        "\n",
        "### 4.1.2 Confusion Matrix\n",
        "\n",
        "A much better way to evaluate the performance of a classifier is to look at the confusion matrix. \n",
        "\n",
        "To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let’s keep it untouched for now (remember that you want to use the test set only at the\n",
        "very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKu0AWz7iG1M"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgehImp6iG1N"
      },
      "source": [
        "Just like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each\n",
        "instance in the training set (“clean” meaning that the prediction is made by a model that never saw the data during training).\n",
        "\n",
        "Now you are ready to get the confusion matrix using the confusion_matrix() function. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REUX0eaHiG1N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cf_matrix = confusion_matrix(y_train_5, y_train_pred)\n",
        "cf_matrix "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI7L6SryiG1O"
      },
      "source": [
        "The confusion matrix we got:\n",
        "\n",
        "|53892    | 687     |  \n",
        "|:-------:|:-------:|\n",
        "|**1891** | **3530**|\n",
        "\n",
        "Your confusion matrix might differ!\n",
        "\n",
        "Each row in a confusion matrix represents an actual, while each column represents a predicted class. The first row of this matrix considers 5 images (the negative class): 53.892 of them were correctly classified as non-5s (true negatives), while the remaining 687 were wrongly classified as 5s (false positives). The second row considers the images of 5s (the positive class): 1.891 were wrongly classified as non-5s (false negatives), while the remaining 3.350 were correctly classified as 5s (true positives). A perfect classifier would have only true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL9TltLJiG1O"
      },
      "outputs": [],
      "source": [
        "y_train_perfect_predictions = y_train_5 # pretend we reached perfection\n",
        "confusion_matrix(y_train_5, y_train_perfect_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDwXoV39iG1P"
      },
      "source": [
        "### 4.1.3 Precision and Recall\n",
        "\n",
        "Scikit-Learn provides several functions to compute classifier metrics, including precision and recall:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMVnsl5LiG1P"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision_score(y_train_5, y_train_pred) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nEsG3xciG1P"
      },
      "outputs": [],
      "source": [
        "recall_score(y_train_5, y_train_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBspBRdmiG1Q"
      },
      "source": [
        "Precision is 83,7% and recall is 65,1%. Thus, we have higher precision than recall. \n",
        "\n",
        "Remember, precision is the measure of how many 5s the model correctly predicted over the amount of correct and incorrect predictions. With a precision of 83,7%, we can state that the model is correct 83,7% of the time. \n",
        "\n",
        "Recall is the measure of how many 5s the model correctly predicted over the total amount of 5s in the dataset. With a recall of 65,1%, we can state that the model detects 65,1% of the true 5s.  \n",
        "\n",
        "Now your 5-detector does not look as shiny as it did when you looked at its accuracy!\n",
        "\n",
        "### 4.1.4 F1 Score\n",
        "To compute the F1 score, simply call the f1_score() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q-3yqrYiG1Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_train_5, y_train_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl5a_R0NiG1R"
      },
      "source": [
        "The F1 score is 73,3%, which is somewhere in the middle of the precision and recall scores. \n",
        "\n",
        "The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught). Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the **precision/recall tradeoff**.\n",
        "\n",
        "### 4.1.5 Precision / Recall Tradeoff \n",
        "\n",
        "To understand this tradeoff, let’s look at how the SGDClassifier makes its classification decisions. For each instance, it computes a score based on a decision function, and if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class. The following figure* shows a few digits positioned from the lowest score on the left to the highest score on the right. Suppose the decision threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and one false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision.\n",
        "\n",
        "\n",
        "![Precision_Recall_Tradeoff](Precision_Recall_Tradeoff.png)\n",
        "\n",
        "Remark*: You can only few the figure if you have saved the Precision_Recall_Tradeoff.png at the same place as this notebook.\n",
        "\n",
        "Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s predict() method, you can call its decision_function() method, which returns a\n",
        "score for each instance, and then make predictions based on those scores using any threshold you want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr9TT0u1iG1R"
      },
      "outputs": [],
      "source": [
        "y_scores = sgd_clf.decision_function([some_digit])\n",
        "y_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDkwHavyiG1S"
      },
      "outputs": [],
      "source": [
        "threshold = 0\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETh0MfA_iG1S"
      },
      "source": [
        "The SGDClassifier uses a threshold equal to 0, so the previous code returns the same result as the predict() method (i.e., True). Let’s raise the threshold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p8UPyzSiG1S"
      },
      "outputs": [],
      "source": [
        "threshold = 8000\n",
        "y_some_digit_pred = (y_scores > threshold)\n",
        "y_some_digit_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Lj6vNFiG1T"
      },
      "source": [
        "This confirms that raising the threshold decreases recall. The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 8.000.\n",
        "\n",
        "Now how do you decide which threshold to use? For this you will first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores instead of predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLR921EUiG1T"
      },
      "outputs": [],
      "source": [
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H1DDf3uiG1T"
      },
      "source": [
        "Now with these scores you can compute precision and recall for all possible thresholds using the precision_recall_curve() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkYZX6KDiG1U"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMHKU-T0iG1U"
      },
      "source": [
        "Finally, you can plot precision and recall as functions of the threshold value using Matplotlib: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4jwvPVBiG1U"
      },
      "outputs": [],
      "source": [
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.figure(figsize=(10, 5), dpi=80) # setting figure size\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "    plt.legend(fontsize=14) \n",
        "    plt.xlabel('Threshold', fontsize=16)\n",
        "\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXiV44F3iG1V"
      },
      "source": [
        "Remark: You may wonder why the precision curve is bumpier than the recall curve in Figure 3-4. The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). To understand why, look back at the previous figure and notice what happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the threshold is increased, which explains why its curve looks smooth.\n",
        "\n",
        "\n",
        "So let’s suppose you decide to aim for 90% precision. Looking the value up that gives you 90% precision from the last plot is difficult You can certainly see that it lies between 0 and 50.000. To be more precise you can search for the lowest threshold that gives you at least 90% precision (np.argmax() will give us the first index of the maximum value, which in this case means the first True value):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWYu4017iG1V"
      },
      "outputs": [],
      "source": [
        "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
        "threshold_90_precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bAPVZiYiG1V"
      },
      "source": [
        "To make predictions (on the training set for now), instead of calling the classifier’s predict() method, you can just run this code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9i2ovStiG1W"
      },
      "outputs": [],
      "source": [
        "y_train_pred_90 = (y_scores >= threshold_90_precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrbh50NQiG1W"
      },
      "source": [
        "Let’s check these predictions’ precision and recall:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQhKtyMviG1W"
      },
      "outputs": [],
      "source": [
        "precision_score(y_train_5, y_train_pred_90), recall_score(y_train_5, y_train_pred_90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sde_yXJWiG1W"
      },
      "source": [
        "Great, you have a 90% precision classifier ! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, and you’re done. Hmm, not so fast. A high-precision classifier is not very useful if its\n",
        "recall is too low!\n",
        "\n",
        "**If someone says “let’s reach 99% precision,” you should ask, “at what recall?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehigd7YSiG1X"
      },
      "source": [
        "# Wednesday\n",
        "\n",
        "## 4.2 Multiclass Classifier\n",
        "\n",
        "Whereas binary classifiers distinguish between two classes, multiclass classifiers (also called multinomial classifiers) can distinguish between more than two classes.\n",
        "\n",
        "Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.\n",
        "\n",
        "For example, one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\n",
        "the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest). \n",
        "\n",
        "Another strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to\n",
        "train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.\n",
        "\n",
        "Some algorithms (such as Support Vector Machine classifiers) scale poorly with the size of the training set, so for these algorithms OvO is preferred since it is faster to train many classifiers on small training sets than training few classifiers on large training sets. For most binary classification algorithms, however, OvA is preferred. \n",
        "\n",
        "Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO). Let’s try this with the SGDClassifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DUDxmvaiG1X"
      },
      "outputs": [],
      "source": [
        "sgd_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
        "sgd_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8CQ7-6iG1X"
      },
      "source": [
        "That was easy! This code trains the SGDClassifier on the training set using the original target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes (y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\n",
        "Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score.\n",
        "\n",
        "If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance and pass a binary classifier to its constructor. For example, this code creates a multiclass classifier using the OvO strategy, based on a SGDClassifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MqbecMYiG1X"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "ovo_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEB1t-L8iG1Y"
      },
      "outputs": [],
      "source": [
        "len(ovo_clf.estimators_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o74jw80ViG1Y"
      },
      "source": [
        "Now of course you want to evaluate the classifier. As usual, you want to use crossvalidation. Let’s evaluate the SGDClassifier’s accuracy using the cross_val_score() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCg9zS8viG1Z"
      },
      "outputs": [],
      "source": [
        "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2V87PPiG1Z"
      },
      "source": [
        "It gets over 85% on all test folds. If you used a random classifier, you would get 10% accuracy, so this is not such a bad score, but you can still do much better. For example, simply scaling the inputs (as discussed in week 10) increases accuracy above\n",
        "89%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOaFtRQxiG1Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
        "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRt9n9AiG1a"
      },
      "source": [
        "### 4.2.1 Confusion Matrix\n",
        "\n",
        "Of course, if this were a real project, you would: explorw data preparation options, try out multiple models, shortlist the best ones and fine-tune their hyperparameters using GridSearchCV, and automating as much as possible, as you did in the previous week. Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.\n",
        "\n",
        "First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOPCAJtxiG1a"
      },
      "outputs": [],
      "source": [
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        "conf_mx "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxuv7tXiG1b"
      },
      "source": [
        "That’s a lot of numbers. It’s often more convenient to look at an image representation of the confusion matrix, using Matplotlib’s matshow() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5c7G9syiG1b"
      },
      "outputs": [],
      "source": [
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SfsDN5ciG1b"
      },
      "source": [
        "This confusion matrix looks fairly good, since most images are on the main diagonal, which means that they were classified correctly. The 5s look slightly darker than the other digits, which could mean that there are fewer images of 5s in the dataset or that\n",
        "the classifier does not perform as well on 5s as on other digits. In fact, you can verify that both are the case.\n",
        "\n",
        "Let’s focus the plot on the errors. First, you need to divide each value in the confusion matrix by the number of images in the corresponding class, so you can compare error rates instead of absolute number of errors (which would make abundant classes look\n",
        "unfairly bad):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPK-nZ1RiG1b"
      },
      "outputs": [],
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YMGszkciG1c"
      },
      "source": [
        "Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDX-4le3iG1c"
      },
      "outputs": [],
      "source": [
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMyPoIQqiG1d"
      },
      "source": [
        "Now you can clearly see the kinds of errors the classifier makes. Remember that rows represent actual classes, while columns represent predicted classes. The column for class 8 is quite bright, which tells you that many images get misclassified as 8s. However, the row for class 8 is not that bad, telling you that actual 8s in general get properly classified as 8s. As you can see, the confusion matrix is not necessarily symmetrical. You can also see that 3s and 5s often get confused (in both directions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neqgJxW9iG1d"
      },
      "source": [
        "### 4.2.2 Precision & Recall\n",
        "\n",
        "In multiclass classification problems, there are no positive or negative classes. But we can compute precision and recall for each class with the sklearn functions precision_score and recall_score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBIKD-gKiG1d"
      },
      "outputs": [],
      "source": [
        "precision_score(y_train, y_train_pred, average=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLr3opJsiG1e"
      },
      "outputs": [],
      "source": [
        "recall_score(y_train, y_train_pred, average=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qqg6BENiG1e"
      },
      "source": [
        "### 4.2.3 Macro, Micro and weighted F1 \n",
        "\n",
        "Macro averaging is perhaps the most straightforward amongst the numerous averaging methods. The macro-averaged F1 score (or macro F1 score) is computed by taking the mean of all the per-class F1 scores. This method treats all classes equally. \n",
        "\n",
        "\n",
        "Micro averaging computes a global average F1 score by counting the sums of the True Positives (TP), False Negatives (FN), and False Positives (FP). RNote, that the micro F1 score is actually the overall accuracy of the model. The reason is that micro-averaging essentially computes the proportion of correctly classified observations out of all observations. If we think about this, this definition is in fact what we use to calculate overall accuracy. \n",
        "\n",
        "The last one is weighted-average F1 score. Unlike Macro F1, it takes the mean of all per-class F1 scores while considering each class’s support. Support refers to the number of actual occurrences of the class in the dataset. For example, the support value of the class 5 is \n",
        "5.421 because there are 5.421 occurrences of that class in the dataset. The ‘weight’ essentially refers to the proportion of each class’s support relative to the sum of all support values. Thus, in case of the class 5, the weight is 5.421 / 60.000. \n",
        "\n",
        "Further readings: [Micro, Macro & Weighted Averages of F1 Score, Clearly Explained](https://towardsdatascience.com/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f) \n",
        "\n",
        "To compute the above scores all in once, you can apply the classification_report function from sklearn.metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EXJ5YfEiG1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "target_names = ['class 0','class 1', 'class 2','class 3','class 4','class 5','class 6','class 7','class 8','class 9']\n",
        "print(classification_report(y_train, y_train_pred,target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqqOt71iG1f"
      },
      "source": [
        "Which average should you choose? \n",
        "\n",
        "In general, if you are working with an imbalanced dataset where all classes are equally important, using the macro average would be a good choice as it treats all classes equally. \n",
        "\n",
        "It means that for our example involving the classification of the digits 0-9, we would use the macro-F1 score. \n",
        "\n",
        "If you have an imbalanced dataset but want to assign greater contribution to classes with more examples in the dataset, then the weighted average is preferred. This is because, in weighted averaging, the contribution of each class to the F1 average is weighted by its size. \n",
        "\n",
        "Suppose you have a balanced dataset and want an easily understandable metric for overall performance regardless of the class. In that case, you can go with accuracy, which is essentially our micro F1 score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjlBil1FiG1f"
      },
      "source": [
        "### 4.2.4 Save Your Model with pickle\n",
        "Finding an accurate machine learning model is not the end of the project. You need to know how to save your model to file and load it later, for example, in order to make predictions.\n",
        "\n",
        "As you saw last week, you can use pickle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLu3nTxNiG1f"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "filename = 'sgd_clf_multiclass.pkl'\n",
        "pickle.dump(sgd_clf, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgjQUtBViG1g"
      },
      "source": [
        "If you want to load you model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGoJz3UviG1g"
      },
      "outputs": [],
      "source": [
        "sgd_clf = pickle.load(open(filename, 'rb')) # load saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLJAxZMBiG1g"
      },
      "source": [
        "# Thursday\n",
        "\n",
        "### Training another model\n",
        "Training a RandomForestClassifier is just as easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWDQKEsHiG1h"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "forest_clf.fit(X_train, y_train)\n",
        "forest_clf.predict([some_digit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQsNzW2iG1h"
      },
      "source": [
        "You can call predict_proba() to get the list of probabilities that the classifier assigned to each instance for each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tJu5BXEiG1h"
      },
      "outputs": [],
      "source": [
        "forest_clf.predict_proba([some_digit])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWX9Tx-diG1h"
      },
      "source": [
        "Our result (note, your numbers might slightly differ): array([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\n",
        "\n",
        "\n",
        "You can see that the classifier is fairly confident about its prediction: we got a 0.9 at the 5th index in the array, that means that the model estimates a 90% probability that the image represents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\n",
        "tively with 1%, 8% and 1% probability.\n",
        "\n",
        "Next we evaluate the Random Forest model on the training set. For that we apply the classification_report function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYBJ35luiG1i"
      },
      "outputs": [],
      "source": [
        "y_train_pred_forest = cross_val_predict(forest_clf, X_train_scaled, y_train, cv=3)\n",
        "print(classification_report(y_train, y_train_pred_forest,target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNBhEiqiG1k"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "The task you were asked is to train a model that predicts the classes, that is, the digits from 0 to 9. In a real world scenario, you would train multiple models, take the best and fine-tune it. We did not fine-tun any model, due to the amount of time we spent on the performance measures of a classification problem. Thus, we pretend that the trained SGD model is our final model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji8UZlxsiG1l"
      },
      "outputs": [],
      "source": [
        "# scale test set\n",
        "X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))\n",
        "\n",
        "# make predictions for the test set\n",
        "y_test_pred = cross_val_predict(sgd_clf, X_test_scaled, y_test, cv=3)\n",
        "\n",
        "# evaluate\n",
        "print(classification_report(y_test, y_test_pred,target_names=target_names))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.5 ('bap': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9127ab915aeeef02e88f7c7310ecae586ccb51d77b915ed104f093bc6474a7bc"
      }
    },
    "colab": {
      "name": "week12_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zxyJZt56iG1E",
        "kuNaiai_iG1L",
        "LDwXoV39iG1P",
        "XBspBRdmiG1Q",
        "wl5a_R0NiG1R",
        "0MRt9n9AiG1a",
        "neqgJxW9iG1d",
        "7Qqg6BENiG1e",
        "JjlBil1FiG1f"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}